Metadata-Version: 2.4
Name: persistent-memory
Version: 0.1.0
Summary: Hierarchical persistent memory system for LLMs
Author: Research Team
Requires-Python: >=3.11
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: temporalio>=1.0.0
Requires-Dist: chromadb>=0.4.0
Requires-Dist: networkx>=3.0
Requires-Dist: openai>=1.0.0
Requires-Dist: typer>=0.9.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: numpy>=1.24.0
Provides-Extra: dev
Requires-Dist: pytest>=7.4.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.21.0; extra == "dev"
Requires-Dist: pytest-cov>=4.1.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"
Requires-Dist: mypy>=1.7.0; extra == "dev"
Requires-Dist: pre-commit>=3.5.0; extra == "dev"
Provides-Extra: docs
Requires-Dist: sphinx>=7.0.0; extra == "docs"
Requires-Dist: sphinx-rtd-theme>=2.0.0; extra == "docs"
Requires-Dist: myst-parser>=2.0.0; extra == "docs"
Dynamic: license-file

# Persistent Context Architecture: Building AI Systems with Infinite Memory

[![CI](https://github.com/[your-org]/persistent-memory-models/workflows/CI/badge.svg)](https://github.com/[your-org]/persistent-memory-models/actions)
[![codecov](https://codecov.io/gh/[your-org]/persistent-memory-models/branch/main/graph/badge.svg)](https://codecov.io/gh/[your-org]/persistent-memory-models)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)

> Transform stateless LLMs into stateful, memory-enabled agents with hierarchical persistent memory.

## The Problem

Current AI systems face a memory wall. Traditional "flat context" approaches (RAG or long context windows) are either forgetful or computationally expensive ($O(n^2)$).

## The Solution: Hierarchical Persistent Memory

We implement a **multi-layered memory system** that mimics human cognition:

```mermaid
graph TD
    User[User Input] --> Router{Context Router}

    subgraph "L1: Working Memory"
        WM[Transformer Context]
        note1[Fast, Expensive, Ephemeral]
    end

    subgraph "L2: Episodic Memory"
        VS[(Vector Store)]
        note2[Semantic Search, ChromaDB]
    end

    subgraph "L3: Semantic Memory"
        KG[(Knowledge Graph)]
        note3[Facts & Relations, NetworkX/Neo4j]
    end

    Router -->|Immediate| WM
    Router -->|Relevant Past| VS
    Router -->|Facts/Entities| KG

    WM -->|Consolidate| VS
    WM -->|Extract Facts| KG
```

## Focus

- **Multi-layered Memory**: Working, Episodic, Semantic, Archive
- **Privacy-First**: Local LLM inference (Ollama)
- **Fast Queries**: O(log n) complexity via HNSW + Graph
- **Durable Workflows**: Temporal orchestration
- **Production-Ready**: Monitoring, testing, CI/CD
- **Type-Safe**: Full type hints with mypy
- **Observable**: Prometheus + Grafana dashboards

## System Architecture

This project is a production-ready implementation of this concept, featuring:

### 1. Core Components

- **Orchestration**: **Temporal** workflows ensure durability. If the system crashes mid-ingestion, it resumes exactly where it left off.
- **Vector Store**: **ChromaDB** for storing embeddings of text chunks (Episodic Memory).
- **Knowledge Graph**: **NetworkX** (MVP) for storing structured entities and relationships (Semantic Memory).
- **LLM Backend**: **Ollama** (Local) or **OpenAI** (Cloud) for cognitive tasks like fact extraction.

### 2. The "Deep Reader" Pipeline

We focus on the "Deep Reader" use case: ingesting entire books and answering complex questions.

1.  **Ingest**: `IngestBookWorkflow` downloads, chunks, and processes text.
2.  **Embed**: Chunks are embedded locally (`all-MiniLM-L6-v2`) and stored in Chroma.
3.  **Extract**: An LLM (Mistral/GPT) reads chunks to extract "Who did what to whom?" facts.
4.  **Query**: A hybrid search combines Vector results (vague similarity) with Graph results (precise relationships).

## Quick Start

### Prerequisites

- Docker & Docker Compose
- Make
- (Optional) Homebrew (for local Ollama)

### 1. Setup Local LLM (Metal/GPU)

To use your Mac's GPU for inference:

```bash
make setup-host-llm
```

This installs Ollama, starts it, and pulls the `mistral` model.

### 2. Start Infrastructure

```bash
make up
```

Starts Temporal, ChromaDB, Postgres, and the App Worker.

### 3. Ingest Data

```bash
# Ingest the first chapter of Pride and Prejudice
docker-compose exec app python -m persistent_memory.cli ingest data/pride_and_prejudice_ch1.txt
```

### 4. Query

```bash
# Ask a question
docker-compose exec app python -m persistent_memory.cli query "Who is Mr. Bennet?"
```

### 5. Access Services

- **API**: http://localhost:8080
- **Grafana**: http://localhost:3000 (admin/admin)
- **Prometheus**: http://localhost:9090
- **Temporal UI**: http://localhost:8088

## ðŸ“Š Performance

| Metric              | Value                  |
| ------------------- | ---------------------- |
| Query Latency (P95) | 280ms                  |
| Ingestion Speed     | 120 chunks/min (Metal) |
| Precision@10        | 0.87                   |
| Recall@10           | 0.72                   |

See [BENCHMARKS.md](docs/BENCHMARKS.md) for detailed performance analysis.

## ðŸ§ª Development

### Run Tests

```bash
make test              # All tests
make test-unit         # Unit tests only
make test-coverage     # With coverage report
```

### Code Quality

```bash
make lint              # Check code
make format            # Auto-format
make pre-commit        # Run all checks
```

### Monitoring

```bash
make logs              # View logs
make metrics           # Open dashboards
```

## ðŸ“š Documentation

- [Deployment Guide](docs/DEPLOYMENT.md)
- [Architecture Decisions](docs/architecture/)
- [Research Paper](docs/PAPER.md)
- [Contributing Guide](CONTRIBUTING.md)

## Directory Structure

```
.
â”œâ”€â”€ src/persistent_memory/     # Core logic
â”‚   â”œâ”€â”€ activities.py          # Temporal activities
â”‚   â”œâ”€â”€ ingestion_workflow.py  # Workflows
â”‚   â”œâ”€â”€ fact_extractor.py      # LLM interface
â”‚   â”œâ”€â”€ context_router.py      # Memory routing
â”‚   â””â”€â”€ api.py                 # FastAPI server
â”œâ”€â”€ tests/                     # Test suite
â”œâ”€â”€ docs/                      # Documentation
â”œâ”€â”€ monitoring/                # Prometheus/Grafana
â””â”€â”€ docker-compose.yml         # Infrastructure
```

## Tech

Built with:

- [Temporal](https://temporal.io) - Durable workflows
- [ChromaDB](https://www.trychroma.com) - Vector database
- [Ollama](https://ollama.ai) - Local LLM inference
- [NetworkX](https://networkx.org) - Graph algorithms
